{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To tackle the project of analyzing duplicates, redundancy, and last modified data for multimedia files, while using strong Python, data science tools, and probabilistic matching (as mentioned in your requirements), we need to develop a structured solution pipeline. The process includes the following components:\n",
        "\n",
        "### 1. **Project Objective and Hypothesis**\n",
        "   - **Objective**: Identify and resolve duplicate or redundant multimedia files based on metadata attributes (like file size, creation date, file type, and content). Ensure the last modified data is captured correctly.\n",
        "   - **Hypothesis**: Duplicate or similar files can be identified based on probabilistic matching and metadata attributes, even if file names or other attributes are slightly altered. This reduces storage footprint and optimizes data archival on magnetic tapes.\n",
        "\n",
        "### 2. **Data Sources**\n",
        "   The data will likely come from multimedia sources like Box, OneDAM, or a custom digital asset management (DAM) system. You'll have metadata available in formats like the ones you uploaded.\n",
        "   \n",
        "   - **Metadata** will contain:\n",
        "     - File names, types, sizes, folder locations, timestamps (created, modified), etc.\n",
        "     - Content signatures like SHA1 hashes (to match exact duplicates).\n",
        "     - Associated tags or information about the file.\n",
        "\n",
        "### 3. **Solution Pipeline**\n",
        "\n",
        "#### Step 1: **Data Collection**\n",
        "   - **APIs**: Integrate with Box, OneDAM, or other storage systems to fetch metadata using APIs (as described in your project).\n",
        "   - Use libraries like:\n",
        "     - `requests` or `httpx` for API calls.\n",
        "     - `pymongo` for MongoDB (since MongoDB is mentioned in your database design).\n",
        "\n",
        "#### Step 2: **Data Cleaning and Preprocessing**\n",
        "   - Normalize metadata fields like `file_name`, `file_type`, `created_date`, etc., to a uniform structure.\n",
        "   - Handle missing or inconsistent metadata by imputing missing values or discarding unnecessary records.\n",
        "   - Convert `created_date` and `modified_date` to comparable formats using `pandas` or `datetime` libraries.\n",
        "   - Convert text fields (like file names) to lowercase and strip extra spaces for better matching.\n",
        "\n",
        "#### Step 3: **Duplicate Detection and Probabilistic Matching**\n",
        "   Use fuzzy matching and probabilistic techniques to identify near-duplicate or exact duplicate files.\n",
        "   \n",
        "   - **Exact Matching**: Use hash values like `SHA1` from the metadata to identify exact duplicates.\n",
        "   - **Near-Duplicate/Probabilistic Matching**: Use a combination of:\n",
        "     - **Fuzzy Matching**: Use libraries like `fuzzywuzzy` or `rapidfuzz` to detect similar file names or content.\n",
        "     - **Record Linkage**: Use `splink` or `dedupe` for probabilistic record linkage.\n",
        "     - **Attributes**: Compare metadata fields like `file_name`, `file_size`, `created_date`, `parentFolder`, etc., to identify duplicates.\n",
        "     - **Example**:\n",
        "       ```python\n",
        "       from fuzzywuzzy import fuzz\n",
        "       ratio = fuzz.ratio(\"filename1.mp4\", \"filename2.mp4\")\n",
        "       ```\n",
        "       For large datasets, prefer more scalable libraries like `splink`.\n",
        "\n",
        "#### Step 4: **Data Storage and Post-Processing**\n",
        "   - Store metadata in **MongoDB** (as specified in the task list).\n",
        "   - Store deduplication analysis and tagged data for files in MongoDB.\n",
        "   - Structure your MongoDB collections similar to the structure in your uploaded file, with fields for `file_id`, `file_name`, `file_size`, etc.\n",
        "   - Use MongoDB queries to retrieve and process data efficiently.\n",
        "\n",
        "#### Step 5: **Archiving to Magnetic Tape**\n",
        "   - Once duplicates and redundant files are detected, generate a final list of unique multimedia files.\n",
        "   - Archive these files to magnetic tapes for long-term storage using tools that support tape archives, such as **LTFS** (Linear Tape File System).\n",
        "\n",
        "### 4. **Libraries to Use**\n",
        "   - **API and Data Handling**:\n",
        "     - `requests`, `httpx`, `pymongo`, `boto3` (if using AWS S3)\n",
        "     - `pandas` for data wrangling and manipulation.\n",
        "   - **Deduplication and Matching**:\n",
        "     - `fuzzywuzzy` or `rapidfuzz` for text matching.\n",
        "     - `splink` or `dedupe` for probabilistic record linkage.\n",
        "   - **Data Analysis**:\n",
        "     - `numpy`, `scipy` for statistical analysis.\n",
        "     - `matplotlib`, `seaborn`, `plotly` for visualization (if insights are needed).\n",
        "   - **Archiving**:\n",
        "     - **LTFS** (Linear Tape File System) for magnetic tape archival.\n",
        "\n",
        "### 5. **Integration with Box/OneDAM**\n",
        "   As per your uploaded project, integrate the system with Box and OneDAM through their respective APIs to retrieve metadata and store results back into the system.\n",
        "\n",
        "   - Use Box APIs for fetching files and metadata.\n",
        "   - Schedule tasks for periodic metadata fetching and storing them in MongoDB.\n",
        "   \n",
        "### 6. **Performance Tuning**\n",
        "   - Optimize the system by using efficient data structures and reducing memory footprint in data processing.\n",
        "   - Use batch processing for handling large metadata volumes.\n",
        "   - Profiling tools like **cProfile** or **memory_profiler** can be helpful.\n",
        "\n",
        "### 7. **End-to-End Workflow**\n",
        "   - **API integration** → **Data Cleaning** → **Fuzzy Matching and Deduplication** → **Data Storage** (MongoDB) → **Post-processing** → **Archiving to Tape**.\n",
        "\n",
        "#### Example Code Outline:\n",
        "```python\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = MongoClient('mongodb://localhost:27017/')\n",
        "db = client['media_db']\n",
        "collection = db['media_metadata']\n",
        "\n",
        "# Fetch data from MongoDB\n",
        "metadata_df = pd.DataFrame(list(collection.find()))\n",
        "\n",
        "# Example of applying fuzzy matching\n",
        "def find_similar_files(df):\n",
        "    df['similar_files'] = df.apply(lambda row: [fuzz.ratio(row['file_name'], x) for x in df['file_name']], axis=1)\n",
        "    return df\n",
        "\n",
        "# Apply deduplication\n",
        "metadata_df = find_similar_files(metadata_df)\n",
        "```\n",
        "\n",
        "### 8. **Testing and Validation**\n",
        "   - Unit testing: Ensure each component works as expected (e.g., fuzzy matching, MongoDB storage).\n",
        "   - Validate the matching engine using a small dataset with known duplicates.\n",
        "  \n",
        "---\n",
        "\n",
        "This solution pipeline focuses on the metadata analysis, deduplication, and archiving process while integrating with storage systems and handling large multimedia datasets effectively."
      ],
      "metadata": {
        "id": "uD-6K9WGi7E0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGfaa0JIi81c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To design a project pipeline for analyzing duplicates, redundant files, and last-modified data for multimedia using techniques like probabilistic matching, deduplication, fuzzy matching, and record linkage, I’ll suggest an approach and outline the steps below. The final goal is to store cleaned and deduplicated data on magnetic tape, which is often used for archiving purposes.\n",
        "\n",
        "### 1. **Project Approach**\n",
        "\n",
        "#### a) **Problem Definition**\n",
        "   - **Objective**: Analyze multimedia files to identify duplicates, redundant data, and determine the most recent versions of files.\n",
        "   - **Dataset**: Multimedia metadata, including details like file names, file sizes, creation and modification dates, etc.\n",
        "   - **Challenges**:\n",
        "     - Inconsistent or slightly different metadata for duplicates.\n",
        "     - Identifying multimedia file duplicates across different folders/locations.\n",
        "     - Ensuring data integrity for archiving on magnetic tape.\n",
        "   \n",
        "#### b) **Hypothesis**\n",
        "   - **Duplicates**: Files with similar metadata such as file size, creation/modification date, and other properties likely represent the same multimedia object.\n",
        "   - **Redundant Data**: Files that are unused or obsolete, potentially older versions of updated files.\n",
        "   - **Last Modified**: The latest version of a file can be determined by analyzing metadata.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Data Pipeline Overview**\n",
        "\n",
        "1. **Data Collection/Integration**\n",
        "   - **Data Sources**: The metadata from systems like Box and OneDAM, as shown in the images, where files are tagged with relevant attributes (created_by, file_name, file_size, etc.).\n",
        "   - **Tools**: Integrate metadata using APIs for platforms like Box and OneDAM.\n",
        "\n",
        "2. **Data Preprocessing**\n",
        "   - **Parsing**: Metadata parsing and extraction from MongoDB, as shown in the provided structure. This includes fields like file_name, file_size, created_date, modified_date, etc.\n",
        "   - **Data Cleaning**: Handle missing, incomplete, or erroneous data entries.\n",
        "   - **Normalization**: Standardize the data types and formats (e.g., ensuring all dates are in a consistent timezone, file sizes are in a uniform unit).\n",
        "   - **Deduplication**: Initial cleaning by eliminating exact duplicates based on metadata.\n",
        "\n",
        "3. **Probabilistic Matching & Record Linkage**\n",
        "   - **Tools**: Use record linkage and deduplication techniques to identify duplicates that are not exact matches.\n",
        "     - **Libraries**:\n",
        "       - `Dedupe` (Python library for fuzzy matching and deduplication)\n",
        "       - `Splink` (for probabilistic record linkage and deduplication)\n",
        "       - `fuzzywuzzy` (for fuzzy string matching)\n",
        "   - **Methodology**:\n",
        "     - Apply probabilistic matching for records with slight variations in file names, sizes, or modification dates.\n",
        "     - Use `fuzzy matching` for metadata fields like `file_name` and `parentFolder`.\n",
        "     - Link records using combinations of fields (file size + created date + file type) with a weighted score.\n",
        "   - **Example**:\n",
        "     - Compare `file_name` using fuzzy matching.\n",
        "     - Compare `file_size` and dates to determine if files are probable duplicates.\n",
        "\n",
        "4. **Analysis of Redundancy & Last Modified**\n",
        "   - Analyze the metadata to find redundant files and determine the most recent version of each file based on `modified_date` and other relevant metadata fields.\n",
        "\n",
        "5. **Storing Clean Data**\n",
        "   - After deduplication and redundancy elimination, save the cleaned metadata and corresponding files for archiving.\n",
        "   - **Output Storage**: Prepare cleaned files for long-term storage on **magnetic tape**.\n",
        "   - **Compression**: Consider compressing files for efficient storage.\n",
        "\n",
        "6. **End-to-End Testing and Validation**\n",
        "   - Create unit test cases and validate the deduplication and matching results using test datasets before applying them on a larger scale.\n",
        "   - Benchmark the accuracy and efficiency of the probabilistic matching model using validation sets.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Pipeline Steps & Tools**\n",
        "\n",
        "#### **Data Sources**:\n",
        "   - **Box and OneDAM**: Fetch metadata from these systems via APIs.\n",
        "   - **MongoDB**: Load metadata into a MongoDB database for processing.\n",
        "\n",
        "#### **Python Libraries**:\n",
        "   - **pandas**: For data manipulation and cleaning.\n",
        "   - **Dedupe**: For identifying duplicates based on probabilistic matching.\n",
        "   - **Splink**: For fuzzy matching and linking records with small variations.\n",
        "   - **fuzzywuzzy**: For string similarity matching (fuzzy matching).\n",
        "   - **pymongo**: For interacting with MongoDB and storing the processed metadata.\n",
        "   - **NumPy**: For efficient data processing.\n",
        "   - **joblib**: For parallel processing if the dataset is large.\n",
        "\n",
        "#### **Data Pipeline Framework**:\n",
        "   - **Apache Airflow**: For scheduling and automating tasks in the pipeline, including fetching metadata, preprocessing, deduplication, and storage.\n",
        "   - **Celery**: For task distribution if running the pipeline on multiple nodes.\n",
        "   - **Docker**: To containerize the application for reproducibility across environments.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Project Stages (Sprints)**\n",
        "\n",
        "#### **Sprint 1**: Design & Integration\n",
        "   - Set up MongoDB database schema based on metadata fields (e.g., `file_name`, `file_size`, etc.).\n",
        "   - Design APIs and use cases for fetching metadata from Box/OneDAM.\n",
        "   - Initial parsing of metadata and exploration of data for insights.\n",
        "\n",
        "#### **Sprint 2**: Development & Testing\n",
        "   - Implement deduplication using probabilistic matching techniques.\n",
        "   - Set up fuzzy matching logic for metadata comparison.\n",
        "   - Develop logic to identify redundant data and determine the most recent file versions.\n",
        "   - Create unit test cases to ensure pipeline accuracy.\n",
        "\n",
        "#### **Sprint 3**: Optimization & Storage\n",
        "   - Optimize deduplication and matching algorithms for large datasets.\n",
        "   - Finalize the pipeline for archiving clean data onto magnetic tapes.\n",
        "   - Performance tuning and resource utilization improvements.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Storage & Archival on Magnetic Tape**\n",
        "\n",
        "   - **Compression Techniques**: Consider compressing multimedia data (e.g., zip, gzip) for efficient storage.\n",
        "   - **Tape Storage Format**: Ensure the file formats are compatible with magnetic tape storage solutions (e.g., LTO tapes).\n",
        "   - **Backup Strategy**: Implement a backup strategy for long-term retention and recovery.\n",
        "\n",
        "---\n",
        "\n",
        "This pipeline combines probabilistic record linkage, data science approaches, and efficient storage mechanisms to meet the project's goals. The use of MongoDB for metadata storage and the various Python libraries for deduplication and fuzzy matching make this a scalable solution for large datasets."
      ],
      "metadata": {
        "id": "jrWldb17kMTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = MongoClient('mongodb://localhost:27017/')\n",
        "db = client['metadata_db']\n",
        "collection = db['files']\n",
        "\n",
        "# Example record structure\n",
        "record = {\n",
        "    'file_name': 'video1.mp4',\n",
        "    'file_size': 1048576,\n",
        "    'created_date': '2023-10-01T12:34:56Z',\n",
        "    'modified_date': '2024-01-01T12:34:56Z',\n",
        "    'file_type': 'video',\n",
        "    'isDuplicate': False\n",
        "}\n",
        "\n",
        "# Insert the record into MongoDB\n",
        "collection.insert_one(record)\n"
      ],
      "metadata": {
        "id": "uO3u9Lg91dmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "\n",
        "# Fetch records from MongoDB\n",
        "metadata = pd.DataFrame(list(collection.find()))\n",
        "\n",
        "# Normalize date fields\n",
        "metadata['created_date'] = metadata['created_date'].apply(parser.parse)\n",
        "metadata['modified_date'] = metadata['modified_date'].apply(parser.parse)\n",
        "\n",
        "# Handle missing values\n",
        "metadata.fillna('', inplace=True)\n"
      ],
      "metadata": {
        "id": "LUPt6QZz1dgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dedupe\n",
        "from unidecode import unidecode\n",
        "\n",
        "# Define the fields for deduplication\n",
        "fields = [\n",
        "    {'field': 'file_name', 'type': 'String'},\n",
        "    {'field': 'file_size', 'type': 'Exact'},\n",
        "    {'field': 'file_type', 'type': 'String'},\n",
        "]\n",
        "\n",
        "# Initialize a Dedupe object\n",
        "deduper = dedupe.Dedupe(fields)\n",
        "\n",
        "# Prepare the data for deduplication\n",
        "data = metadata.to_dict('index')\n",
        "\n",
        "# Sample data for training the deduper\n",
        "deduper.sample(data, 10000)\n",
        "\n",
        "# Active learning loop for training the deduper\n",
        "deduper.train()\n",
        "\n",
        "# Blocking and linking the duplicates\n",
        "clustered_dupes = deduper.partition(data, threshold=0.5)\n",
        "\n",
        "# Mark duplicates in MongoDB\n",
        "for cluster_id, records in clustered_dupes.items():\n",
        "    for record in records:\n",
        "        collection.update_one({'_id': record['_id']}, {'$set': {'isDuplicate': True}})\n"
      ],
      "metadata": {
        "id": "kf39OlNI1dd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by modified date and group by file_name\n",
        "metadata.sort_values('modified_date', ascending=False, inplace=True)\n",
        "metadata['is_latest'] = metadata.duplicated(subset=['file_name'], keep='first')\n",
        "\n",
        "# Mark older versions as redundant\n",
        "metadata.loc[metadata['is_latest'] == False, 'isDuplicate'] = True\n",
        "\n",
        "# Update MongoDB\n",
        "for index, row in metadata.iterrows():\n",
        "    collection.update_one({'_id': row['_id']}, {'$set': {'isDuplicate': row['isDuplicate']}})\n"
      ],
      "metadata": {
        "id": "6HS8L1PM1n_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudo code for archiving to magnetic tape\n",
        "def archive_to_tape(file_path, tape_device):\n",
        "    # Use system commands or APIs to write files to tape\n",
        "    os.system(f\"tar -cvf {tape_device} {file_path}\")\n",
        "\n",
        "# Archive all non-duplicate files\n",
        "for record in collection.find({'isDuplicate': False}):\n",
        "    archive_to_tape(record['file_path'], '/dev/tape_device')\n"
      ],
      "metadata": {
        "id": "HrSvlR8i1n6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Example of fuzzy matching for file names\n",
        "def is_similar_name(name1, name2):\n",
        "    return fuzz.ratio(name1, name2) > 85  # Return True if similarity is above 85%\n",
        "\n",
        "# Test similarity\n",
        "print(is_similar_name(\"video1_final.mp4\", \"video1.mp4\"))  # Likely to be duplicates\n"
      ],
      "metadata": {
        "id": "pqaCsZ451n3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DPGLE2p-1n0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "\n",
        "# Define a DAG (Directed Acyclic Graph)\n",
        "with DAG('multimedia_deduplication_pipeline', start_date=datetime(2024, 10, 15), schedule_interval='@daily') as dag:\n",
        "\n",
        "    fetch_metadata = PythonOperator(\n",
        "        task_id='fetch_metadata',\n",
        "        python_callable=fetch_metadata_from_box_onedam\n",
        "    )\n",
        "\n",
        "    preprocess_data = PythonOperator(\n",
        "        task_id='preprocess_data',\n",
        "        python_callable=preprocess_metadata\n",
        "    )\n",
        "\n",
        "    deduplicate_data = PythonOperator(\n",
        "        task_id='deduplicate_data',\n",
        "        python_callable=perform_deduplication\n",
        "    )\n",
        "\n",
        "    archive_data = PythonOperator(\n",
        "        task_id='archive_data',\n",
        "        python_callable=archive_to_magnetic_tape\n",
        "    )\n",
        "\n",
        "    # Define task dependencies\n",
        "    fetch_metadata >> preprocess_data >> deduplicate_data >> archive_data\n"
      ],
      "metadata": {
        "id": "4nggvekp1u5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate a SHA-1 checksum for audio files stored in an S3 bucket without downloading the files locally, you can use the AWS SDK in combination with the **ETag** or initiate an operation directly on the S3 object. However, by default, S3 calculates and provides an **MD5 checksum** (ETag) for files, not SHA-1.\n",
        "\n",
        "Unfortunately, AWS S3 doesn't natively provide SHA-1 checksums out-of-the-box (for example, in the metadata or ETag). You will need to implement this checksum calculation on the file either on-the-fly during upload or using AWS services like **AWS Lambda** or **AWS SDKs**.\n",
        "\n",
        "### Approach 1: Using AWS SDK with S3's ETag (not SHA-1 but MD5)\n",
        "- S3’s **ETag** is usually an MD5 checksum (with caveats for multipart uploads). However, this won't help since Box uses **SHA-1**.\n",
        "  \n",
        "### Approach 2: Using AWS Lambda for SHA-1 checksum generation (without downloading the file)\n",
        "\n",
        "You can create an **AWS Lambda function** that reads files directly from S3, computes the SHA-1 checksum, and returns or stores it back in the S3 bucket or another service.\n",
        "\n",
        "Here’s a **Python AWS Lambda** example using **Boto3** and **hashlib** to calculate SHA-1 checksums for files in S3:\n",
        "\n",
        "1. **Create an S3-triggered Lambda function** that runs every time a new file is uploaded or on-demand.\n",
        "\n",
        "2. **Code Example (Python)**:\n",
        "   ```python\n",
        "   import hashlib\n",
        "   import boto3\n",
        "   import botocore\n",
        "\n",
        "   s3_client = boto3.client('s3')\n",
        "\n",
        "   def lambda_handler(event, context):\n",
        "       bucket_name = event['Records'][0]['s3']['bucket']['name']\n",
        "       object_key = event['Records'][0]['s3']['object']['key']\n",
        "\n",
        "       # Fetch the object from S3\n",
        "       try:\n",
        "           s3_object = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
        "           file_content = s3_object['Body'].read()\n",
        "\n",
        "           # Compute SHA-1 checksum\n",
        "           sha1_checksum = hashlib.sha1(file_content).hexdigest()\n",
        "\n",
        "           print(f\"SHA-1 checksum for {object_key}: {sha1_checksum}\")\n",
        "\n",
        "           # Optionally store the SHA-1 checksum as metadata or in a DynamoDB table, etc.\n",
        "\n",
        "           return {\n",
        "               'statusCode': 200,\n",
        "               'sha1_checksum': sha1_checksum\n",
        "           }\n",
        "       except botocore.exceptions.ClientError as e:\n",
        "           print(f\"Error fetching the object {object_key}: {e}\")\n",
        "           return {\n",
        "               'statusCode': 500,\n",
        "               'error': str(e)\n",
        "           }\n",
        "   ```\n",
        "\n",
        "   - **Explanation**:\n",
        "     - The Lambda function is triggered when an object is uploaded to S3.\n",
        "     - It retrieves the file content and calculates the **SHA-1 checksum** using the `hashlib.sha1()` function.\n",
        "     - The checksum can then be printed, stored as metadata, or returned as part of the response.\n",
        "\n",
        "3. **Advantages**:\n",
        "   - You don't need to download the file manually; the Lambda reads it directly from S3.\n",
        "   - The function can handle large files without requiring manual download, though there are **Lambda size and time limits**.\n",
        "\n",
        "4. **Add Metadata (optional)**:\n",
        "   You can also update the S3 object to store the SHA-1 checksum as metadata if needed:\n",
        "   ```python\n",
        "   # Add the checksum to the metadata of the S3 object\n",
        "   s3_client.put_object(\n",
        "       Bucket=bucket_name,\n",
        "       Key=object_key,\n",
        "       Metadata={'SHA1Checksum': sha1_checksum}\n",
        "   )\n",
        "   ```\n",
        "\n",
        "### Approach 3: Direct Calculation during Upload\n",
        "If you’re uploading files using the AWS SDK, you can calculate the **SHA-1 checksum** before or during the upload and store it as metadata. Here’s how you can do it using **Boto3** in Python:\n",
        "\n",
        "```python\n",
        "import hashlib\n",
        "import boto3\n",
        "\n",
        "def upload_with_sha1(bucket_name, file_path, object_key):\n",
        "    s3_client = boto3.client('s3')\n",
        "    \n",
        "    # Calculate SHA-1 checksum\n",
        "    sha1 = hashlib.sha1()\n",
        "    \n",
        "    with open(file_path, 'rb') as f:\n",
        "        while chunk := f.read(8192):\n",
        "            sha1.update(chunk)\n",
        "    sha1_checksum = sha1.hexdigest()\n",
        "    \n",
        "    # Upload the file to S3 and store the SHA-1 checksum as metadata\n",
        "    s3_client.upload_file(\n",
        "        Filename=file_path,\n",
        "        Bucket=bucket_name,\n",
        "        Key=object_key,\n",
        "        ExtraArgs={'Metadata': {'SHA1Checksum': sha1_checksum}}\n",
        "    )\n",
        "\n",
        "    print(f\"File {file_path} uploaded to {bucket_name}/{object_key} with SHA-1 checksum: {sha1_checksum}\")\n",
        "```\n",
        "\n",
        "### Documentation for Reference\n",
        "1. **Boto3 S3 Documentation** (Python SDK for AWS):\n",
        "   - [Boto3 S3 Client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file)\n",
        "   - [AWS SDK for S3 Object Operations](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Object.html)\n",
        "\n",
        "2. **hashlib documentation**:\n",
        "   - [Python hashlib for SHA-1](https://docs.python.org/3/library/hashlib.html#hashlib.sha1)\n",
        "\n",
        "### Verifying SHA-1 Algorithm\n",
        "Both Box and AWS use the **SHA-1 algorithm as defined by the FIPS PUB 180-4** standard, ensuring that they should compute identical SHA-1 hashes for the same content.\n",
        "\n",
        "If you want a direct approach from AWS without running Lambda, there's no built-in functionality to generate SHA-1 checksums directly on S3 objects without downloading or accessing the object content. The best option would be a Lambda, or you could compute the checksum before uploading and store it as metadata.\n",
        "\n",
        "This should give you a clear path to compute SHA-1 for your S3 audio files and ensure compatibility with Box's checksum validation process."
      ],
      "metadata": {
        "id": "VuWadUn47Oxw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JjitDu-U7PVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the problem of generating a SHA-1 checksum for S3 objects (like audio files) using AWS SDKs, integrating metadata from Box, and analyzing redundant or duplicated files, you can approach this in stages by breaking it down into manageable components.\n",
        "\n",
        "Given your project requirements, I will propose a pipeline that can help automate the checksum generation, handle duplicates, and integrate data from S3 and Box. I'll also cover how splink and probabilistic matching can help in deduplication and redundant file analysis."
      ],
      "metadata": {
        "id": "OJ-qJz5E8dgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline Breakdown\n",
        "\n",
        "    Fetch Files from S3 and Compute SHA-1 Checksums\n",
        "        You can use AWS SDK (like Boto3 in Python) to fetch files from S3 and compute the SHA-1 checksum for each file.\n",
        "        Ensure that you handle this without downloading the file locally using streams and AWS’s direct object access.\n",
        "        Option 1: Implement a Lambda function that computes the checksum when a file is uploaded to S3, or process all existing files.\n",
        "\n",
        "    Code Example (for Python using Boto3):"
      ],
      "metadata": {
        "id": "DhOfmAbO8l0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "import boto3\n",
        "\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def generate_sha1_from_s3(bucket, key):\n",
        "    # Download the file in memory and calculate the SHA-1 checksum\n",
        "    s3_object = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    file_content = s3_object['Body'].read()\n",
        "\n",
        "    # Compute SHA-1\n",
        "    sha1_checksum = hashlib.sha1(file_content).hexdigest()\n",
        "    return sha1_checksum\n",
        "\n",
        "# Example usage\n",
        "bucket_name = \"your-s3-bucket\"\n",
        "file_key = \"path/to/your/audio.mp3\"\n",
        "checksum = generate_sha1_from_s3(bucket_name, file_key)\n",
        "print(f\"The SHA-1 checksum of the file is: {checksum}\")\n"
      ],
      "metadata": {
        "id": "rBYrHUIU8eXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store or Compare Metadata\n",
        "\n",
        "    Once you calculate the SHA-1 checksum, store it in S3 metadata, or in a DynamoDB database, for future comparisons with Box's metadata.\n",
        "    Box’s metadata includes a SHA-1 field, which you can directly compare with S3’s computed checksum.\n",
        "    Fetch Box file metadata using Box SDK or API and match it against the checksum calculated in S3.\n",
        "\n",
        "Probabilistic Matching & Duplicate Detection\n",
        "\n",
        "    Using the metadata (SHA-1 checksums, file names, sizes, etc.) from both S3 and Box, you can run duplicate detection.\n",
        "    For this, you can use the splink library for probabilistic record linkage (since you are working with potentially large datasets).\n",
        "    Splink will help match files from Box and S3 using fuzzy matching, dedupe logic, or probabilistic approaches when exact matches (such as SHA-1 hashes) are not found.\n",
        "\n",
        "Hypothesis: Duplicate or redundant files are likely to have matching SHA-1 checksums or near-identical metadata (size, timestamp)."
      ],
      "metadata": {
        "id": "IgIAirFX8tYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import splink\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have metadata for files from both S3 and Box\n",
        "s3_data = pd.DataFrame([\n",
        "    {'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'file_name': 'audio2.mp3', 'sha1': 'XYZ456', 'size': 67890}\n",
        "])\n",
        "\n",
        "box_data = pd.DataFrame([\n",
        "    {'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'file_name': 'audio_duplicate.mp3', 'sha1': 'ABC123', 'size': 12345}\n",
        "])\n",
        "\n",
        "# Define the model\n",
        "settings = {\n",
        "    \"link_type\": \"dedupe_only\",\n",
        "    \"blocking_rules_to_generate_predictions\": [\"l.file_name = r.file_name\"],\n",
        "    \"comparison_columns\": [\n",
        "        {\"col_name\": \"sha1\", \"num_levels\": 2}\n",
        "    ]\n",
        "}\n",
        "\n",
        "linker = splink.Splink(settings, [s3_data, box_data])\n",
        "results = linker.get_match_candidates()\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "4Nca6iTy8uOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata Storage & Analysis\n",
        "\n",
        "    For efficient storage and analysis of metadata, you can use MongoDB or any NoSQL database.\n",
        "    Store all metadata (file names, checksums, timestamps, etc.) from S3 and Box for further analysis, comparison, and deduplication.\n",
        "    Use MongoDB queries to fetch duplicates, or files that haven't been matched, and analyze redundancy based on user requirements.\n",
        "\n",
        "Big Data Scaling\n",
        "\n",
        "    If your dataset is large, you can use AWS Glue for ETL processing, AWS Athena for querying metadata at scale, and AWS S3 Select to optimize data extraction without needing to pull large files.\n",
        "    For batch processing and running the deduplication at scale, consider using AWS EMR with Spark and PySpark."
      ],
      "metadata": {
        "id": "jov1Sdpk84ic"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eD8QN79F9QJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To address the problem of generating a SHA-1 checksum for S3 objects (like audio files) using AWS SDKs, integrating metadata from Box, and analyzing redundant or duplicated files, you can approach this in stages by breaking it down into manageable components.\n",
        "\n",
        "Given your project requirements, I will propose a **pipeline** that can help automate the checksum generation, handle duplicates, and integrate data from S3 and Box. I'll also cover how **splink** and probabilistic matching can help in deduplication and redundant file analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### Pipeline Breakdown\n",
        "\n",
        "1. **Fetch Files from S3 and Compute SHA-1 Checksums**\n",
        "    - You can use AWS SDK (like **Boto3** in Python) to fetch files from S3 and compute the **SHA-1** checksum for each file.\n",
        "    - Ensure that you handle this without downloading the file locally using streams and AWS’s direct object access.\n",
        "    - **Option 1**: Implement a Lambda function that computes the checksum when a file is uploaded to S3, or process all existing files.\n",
        "\n",
        "    **Code Example** (for Python using Boto3):\n",
        "    ```python\n",
        "    import hashlib\n",
        "    import boto3\n",
        "\n",
        "    s3_client = boto3.client('s3')\n",
        "\n",
        "    def generate_sha1_from_s3(bucket, key):\n",
        "        # Download the file in memory and calculate the SHA-1 checksum\n",
        "        s3_object = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "        file_content = s3_object['Body'].read()\n",
        "        \n",
        "        # Compute SHA-1\n",
        "        sha1_checksum = hashlib.sha1(file_content).hexdigest()\n",
        "        return sha1_checksum\n",
        "\n",
        "    # Example usage\n",
        "    bucket_name = \"your-s3-bucket\"\n",
        "    file_key = \"path/to/your/audio.mp3\"\n",
        "    checksum = generate_sha1_from_s3(bucket_name, file_key)\n",
        "    print(f\"The SHA-1 checksum of the file is: {checksum}\")\n",
        "    ```\n",
        "\n",
        "2. **Store or Compare Metadata**\n",
        "    - Once you calculate the SHA-1 checksum, store it in S3 metadata, or in a DynamoDB database, for future comparisons with Box's metadata.\n",
        "    - Box’s metadata includes a **SHA-1** field, which you can directly compare with S3’s computed checksum.\n",
        "    - Fetch Box file metadata using **Box SDK** or API and match it against the checksum calculated in S3.\n",
        "\n",
        "3. **Probabilistic Matching & Duplicate Detection**\n",
        "    - Using the metadata (SHA-1 checksums, file names, sizes, etc.) from both S3 and Box, you can run **duplicate detection**.\n",
        "    - For this, you can use the **splink** library for **probabilistic record linkage** (since you are working with potentially large datasets).\n",
        "    - **Splink** will help match files from Box and S3 using **fuzzy matching**, **dedupe** logic, or **probabilistic approaches** when exact matches (such as SHA-1 hashes) are not found.\n",
        "    \n",
        "    **Hypothesis**: Duplicate or redundant files are likely to have matching SHA-1 checksums or near-identical metadata (size, timestamp).\n",
        "    \n",
        "    **Example**:\n",
        "    ```python\n",
        "    import splink\n",
        "    import pandas as pd\n",
        "\n",
        "    # Assuming you have metadata for files from both S3 and Box\n",
        "    s3_data = pd.DataFrame([\n",
        "        {'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "        {'file_name': 'audio2.mp3', 'sha1': 'XYZ456', 'size': 67890}\n",
        "    ])\n",
        "\n",
        "    box_data = pd.DataFrame([\n",
        "        {'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "        {'file_name': 'audio_duplicate.mp3', 'sha1': 'ABC123', 'size': 12345}\n",
        "    ])\n",
        "\n",
        "    # Define the model\n",
        "    settings = {\n",
        "        \"link_type\": \"dedupe_only\",\n",
        "        \"blocking_rules_to_generate_predictions\": [\"l.file_name = r.file_name\"],\n",
        "        \"comparison_columns\": [\n",
        "            {\"col_name\": \"sha1\", \"num_levels\": 2}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    linker = splink.Splink(settings, [s3_data, box_data])\n",
        "    results = linker.get_match_candidates()\n",
        "    \n",
        "    print(results)\n",
        "    ```\n",
        "\n",
        "4. **Metadata Storage & Analysis**\n",
        "    - For efficient storage and analysis of metadata, you can use **MongoDB** or any NoSQL database.\n",
        "    - Store all metadata (file names, checksums, timestamps, etc.) from S3 and Box for further analysis, comparison, and deduplication.\n",
        "    - Use MongoDB queries to fetch duplicates, or files that haven't been matched, and analyze redundancy based on user requirements.\n",
        "\n",
        "5. **Big Data Scaling**\n",
        "    - If your dataset is large, you can use **AWS Glue** for ETL processing, **AWS Athena** for querying metadata at scale, and **AWS S3 Select** to optimize data extraction without needing to pull large files.\n",
        "    - For **batch processing** and running the deduplication at scale, consider using **AWS EMR** with Spark and PySpark.\n",
        "\n",
        "---\n",
        "\n",
        "### Complete Pipeline\n",
        "\n",
        "1. **Data Ingestion**:\n",
        "   - Fetch audio file metadata from both S3 and Box.\n",
        "   - Compute the **SHA-1 checksum** for files in S3 (using Lambda or direct Boto3 calls).\n",
        "   \n",
        "2. **Metadata Storage**:\n",
        "   - Store the metadata in **MongoDB**, including file name, size, creation date, and SHA-1 checksums from both sources.\n",
        "\n",
        "3. **Probabilistic Matching & Deduplication**:\n",
        "   - Use **splink** to match and deduplicate records between S3 and Box.\n",
        "   - Use **fuzzy logic** and exact matches based on SHA-1, file name, and file size.\n",
        "\n",
        "4. **Duplicate Identification**:\n",
        "   - Tag files as duplicates if the checksum matches, or if the probabilistic model identifies them as near duplicates.\n",
        "   \n",
        "5. **Post-Processing and Analysis**:\n",
        "   - Use Python and Pandas for further analysis and reporting, identifying redundant files and generating reports for stakeholders.\n",
        "\n",
        "---\n",
        "\n",
        "### Tools and Technologies\n",
        "1. **AWS SDK (Boto3)** - for S3 operations.\n",
        "2. **Box SDK** - for fetching metadata from Box.\n",
        "3. **hashlib** - for SHA-1 checksum generation.\n",
        "4. **splink** - for probabilistic matching and deduplication.\n",
        "5. **MongoDB** - for metadata storage and querying.\n",
        "6. **Python (Pandas)** - for processing and analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### Hypothesis to Result Flow\n",
        "1. **Hypothesis**: Files with identical SHA-1 checksums or similar metadata (name, size) are duplicates.\n",
        "2. **Steps**:\n",
        "   - Fetch metadata from S3 and Box.\n",
        "   - Compute SHA-1 checksum for S3 files.\n",
        "   - Use **splink** for probabilistic matching.\n",
        "3. **Result**: Identified duplicate files can be tagged and removed or analyzed for redundancy.\n",
        "\n",
        "This pipeline ensures end-to-end coverage of metadata processing, deduplication, and checksum validation, providing a solid foundation to complete your project."
      ],
      "metadata": {
        "id": "bNhh_WTZ9FXi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FD2pIP9D84UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjcTOX2E84Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the solution using the mentioned tools and technologies, I'll walk you through how each can be applied to solve the problem of **deduplication** and **metadata comparison** between **AWS S3** and **Box**, including computing **SHA-1 checksums**, managing metadata, and **probabilistic matching**.\n",
        "\n",
        "Here’s a step-by-step guide on how to use these tools effectively:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **AWS SDK (Boto3) for S3 Operations**\n",
        "\n",
        "Boto3 is the AWS SDK for Python. We’ll use it to interact with AWS S3—specifically to fetch files, compute their SHA-1 checksum, and work with metadata.\n",
        "\n",
        "#### Use Case:\n",
        "- Fetch audio files from your **S3 bucket** and retrieve the metadata.\n",
        "- Compute and store the **SHA-1 checksum** for each file.\n",
        "\n",
        "#### Example Code:\n",
        "```python\n",
        "import boto3\n",
        "import hashlib\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def fetch_s3_metadata(bucket, key):\n",
        "    \"\"\"\n",
        "    Fetch file content and metadata from an S3 bucket.\n",
        "    \"\"\"\n",
        "    # Fetch the file from S3\n",
        "    s3_object = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    file_content = s3_object['Body'].read()  # File content for checksum\n",
        "    metadata = s3_object['Metadata']  # Existing metadata if any\n",
        "    return file_content, metadata\n",
        "\n",
        "def generate_sha1_checksum(file_content):\n",
        "    \"\"\"\n",
        "    Generate SHA-1 checksum for the file content.\n",
        "    \"\"\"\n",
        "    sha1 = hashlib.sha1()\n",
        "    sha1.update(file_content)\n",
        "    return sha1.hexdigest()\n",
        "\n",
        "bucket_name = \"your-s3-bucket\"\n",
        "file_key = \"path/to/your/file.mp3\"\n",
        "\n",
        "# Get file content from S3 and calculate the SHA-1 checksum\n",
        "file_content, metadata = fetch_s3_metadata(bucket_name, file_key)\n",
        "checksum = generate_sha1_checksum(file_content)\n",
        "\n",
        "print(f\"SHA-1 Checksum for {file_key}: {checksum}\")\n",
        "```\n",
        "\n",
        "#### Steps:\n",
        "1. **Fetch files from S3**: Use `get_object()` to retrieve the file and metadata from S3.\n",
        "2. **Compute SHA-1 Checksum**: Use `hashlib` to generate the checksum for deduplication purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Box SDK for Metadata Fetching**\n",
        "\n",
        "The **Box SDK** allows you to interact with Box to fetch metadata for files, including the **SHA-1 checksum** Box provides.\n",
        "\n",
        "#### Use Case:\n",
        "- Fetch metadata (including the SHA-1 checksum) from Box for comparison with files in S3.\n",
        "\n",
        "#### Example Code:\n",
        "```python\n",
        "from boxsdk import Client, OAuth2\n",
        "\n",
        "# Authenticate with Box\n",
        "oauth2 = OAuth2(\n",
        "    client_id='your-client-id',\n",
        "    client_secret='your-client-secret',\n",
        "    access_token='your-access-token'\n",
        ")\n",
        "client = Client(oauth2)\n",
        "\n",
        "def get_box_file_metadata(file_id):\n",
        "    \"\"\"\n",
        "    Fetch metadata (including SHA-1) from Box for the given file.\n",
        "    \"\"\"\n",
        "    box_file = client.file(file_id).get()\n",
        "    sha1_checksum = box_file.sha1\n",
        "    file_name = box_file.name\n",
        "    file_size = box_file.size\n",
        "    return {\"name\": file_name, \"size\": file_size, \"sha1\": sha1_checksum}\n",
        "\n",
        "box_file_id = \"1234567890\"\n",
        "box_metadata = get_box_file_metadata(box_file_id)\n",
        "print(f\"Box File Metadata: {box_metadata}\")\n",
        "```\n",
        "\n",
        "#### Steps:\n",
        "1. **Authenticate with Box**: Use OAuth2 credentials to authenticate with the Box API.\n",
        "2. **Fetch Metadata**: Use the Box SDK to retrieve metadata, including the SHA-1 checksum.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Hashlib for SHA-1 Checksum Generation**\n",
        "\n",
        "**Hashlib** is used to compute the **SHA-1 checksum** for the files fetched from **S3**.\n",
        "\n",
        "#### Use Case:\n",
        "- After fetching the file from S3, compute the **SHA-1 checksum** to compare with Box files.\n",
        "\n",
        "#### Example:\n",
        "```python\n",
        "import hashlib\n",
        "\n",
        "def generate_sha1_checksum(file_content):\n",
        "    sha1 = hashlib.sha1()\n",
        "    sha1.update(file_content)\n",
        "    return sha1.hexdigest()\n",
        "\n",
        "file_content = b\"Sample file content\"\n",
        "checksum = generate_sha1_checksum(file_content)\n",
        "print(f\"SHA-1 checksum: {checksum}\")\n",
        "```\n",
        "\n",
        "#### Steps:\n",
        "1. **Compute Checksum**: Use `hashlib` to generate SHA-1 hash for any file or byte content.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Splink for Probabilistic Matching and Deduplication**\n",
        "\n",
        "**Splink** is a library for probabilistic record linkage and deduplication. It helps you match files between S3 and Box, even when exact matches are not possible (e.g., mismatched file names).\n",
        "\n",
        "#### Use Case:\n",
        "- Use **Splink** to compare metadata from S3 and Box and identify duplicates or similar files based on checksum, file name, or size.\n",
        "\n",
        "#### Example Code:\n",
        "```python\n",
        "import pandas as pd\n",
        "from splink import Splink\n",
        "\n",
        "# Example data from S3 and Box\n",
        "s3_data = pd.DataFrame([\n",
        "    {'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'file_name': 'audio2.mp3', 'sha1': 'XYZ456', 'size': 67890}\n",
        "])\n",
        "\n",
        "box_data = pd.DataFrame([\n",
        "    {'file_name': 'audio_duplicate.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'file_name': 'audio2.mp3', 'sha1': 'XYZ456', 'size': 67890}\n",
        "])\n",
        "\n",
        "# Linkage settings\n",
        "settings = {\n",
        "    \"link_type\": \"dedupe_only\",\n",
        "    \"comparison_columns\": [\n",
        "        {\"col_name\": \"file_name\", \"num_levels\": 2},\n",
        "        {\"col_name\": \"sha1\", \"num_levels\": 2},\n",
        "        {\"col_name\": \"size\", \"num_levels\": 2}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Link S3 and Box data\n",
        "linker = Splink(settings, [s3_data, box_data])\n",
        "results = linker.get_match_candidates()\n",
        "print(results)\n",
        "```\n",
        "\n",
        "#### Steps:\n",
        "1. **Prepare data**: Extract metadata from both S3 and Box and convert it to a DataFrame.\n",
        "2. **Define matching settings**: Use Splink to compare file name, SHA-1 checksum, and size for deduplication.\n",
        "3. **Run Matching**: Execute Splink to find exact or probabilistic matches.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **MongoDB for Metadata Storage**\n",
        "\n",
        "Store all metadata from S3 and Box in **MongoDB** for querying and deduplication analysis.\n",
        "\n",
        "#### Use Case:\n",
        "- Use MongoDB to store the file metadata from S3 and Box, along with computed SHA-1 checksums, for easy querying and analysis.\n",
        "\n",
        "#### Example Code:\n",
        "```python\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = MongoClient('mongodb://localhost:27017/')\n",
        "db = client['file_metadata_db']\n",
        "collection = db['file_metadata']\n",
        "\n",
        "# Insert metadata from S3\n",
        "s3_metadata = {\n",
        "    \"file_name\": \"audio1.mp3\",\n",
        "    \"sha1\": \"ABC123\",\n",
        "    \"size\": 12345,\n",
        "    \"source\": \"S3\"\n",
        "}\n",
        "collection.insert_one(s3_metadata)\n",
        "\n",
        "# Insert metadata from Box\n",
        "box_metadata = {\n",
        "    \"file_name\": \"audio_duplicate.mp3\",\n",
        "    \"sha1\": \"ABC123\",\n",
        "    \"size\": 12345,\n",
        "    \"source\": \"Box\"\n",
        "}\n",
        "collection.insert_one(box_metadata)\n",
        "\n",
        "# Query for duplicates based on SHA-1 checksum\n",
        "duplicates = collection.find({\"sha1\": \"ABC123\"})\n",
        "for doc in duplicates:\n",
        "    print(doc)\n",
        "```\n",
        "\n",
        "#### Steps:\n",
        "1. **Insert Metadata**: After calculating SHA-1 checksums, store file metadata from S3 and Box into MongoDB.\n",
        "2. **Query Data**: Use MongoDB queries to search for duplicates or near-duplicates based on SHA-1 or other criteria.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Python (Pandas) for Processing and Analysis**\n",
        "\n",
        "Pandas will help you manipulate and analyze metadata from both **S3** and **Box**. After identifying duplicates with **Splink** or SHA-1 checksum comparison, you can use **Pandas** for further processing.\n",
        "\n",
        "#### Use Case:\n",
        "- Use **Pandas** to analyze the file metadata, detect duplicates, and generate reports.\n",
        "\n",
        "#### Example Code:\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Example data from S3 and Box\n",
        "data = [\n",
        "    {'source': 'S3', 'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'source': 'Box', 'file_name': 'audio_duplicate.mp3', 'sha1': 'ABC123', 'size': 12345}\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Find duplicate files based on SHA-1\n",
        "duplicates = df[df.duplicated(subset='sha1', keep=False)]\n",
        "print(duplicates)\n",
        "```\n",
        "\n",
        "#### Steps:\n",
        "1. **Load Metadata**: Use Pandas to read and manipulate metadata from S3 and Box.\n",
        "2. **Analyze Duplicates**: Use Pandas to detect duplicates based on SHA-1 or other attributes.\n",
        "3. **Generate Reports**: Summarize the results (e.g., how many duplicates were found) for reporting purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Workflow\n",
        "\n",
        "1. **S3 File Handling**: Use **AWS SDK (Boto3)** to fetch file metadata and generate **SHA-1** checksums.\n",
        "2. **Box Metadata**: Use **Box SDK** to fetch file metadata from Box.\n",
        "3. **Checksum Comparison**: Compare the **SHA-1 checksums** between S3 and Box files.\n",
        "4."
      ],
      "metadata": {
        "id": "0MXf2W4_D5BR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS SDK (Boto3) for S3 Operations\n",
        "\n",
        "Boto3 is the AWS SDK for Python. We’ll use it to interact with AWS S3—specifically to fetch files, compute their SHA-1 checksum, and work with metadata.\n",
        "Use Case:\n",
        "\n",
        "    Fetch audio files from your S3 bucket and retrieve the metadata.\n",
        "    Compute and store the SHA-1 checksum for each file.\n"
      ],
      "metadata": {
        "id": "AFaz43-iTu9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import hashlib\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def fetch_s3_metadata(bucket, key):\n",
        "    \"\"\"\n",
        "    Fetch file content and metadata from an S3 bucket.\n",
        "    \"\"\"\n",
        "    # Fetch the file from S3\n",
        "    s3_object = s3_client.get_object(Bucket=bucket, Key=key)\n",
        "    file_content = s3_object['Body'].read()  # File content for checksum\n",
        "    metadata = s3_object['Metadata']  # Existing metadata if any\n",
        "    return file_content, metadata\n",
        "\n",
        "def generate_sha1_checksum(file_content):\n",
        "    \"\"\"\n",
        "    Generate SHA-1 checksum for the file content.\n",
        "    \"\"\"\n",
        "    sha1 = hashlib.sha1()\n",
        "    sha1.update(file_content)\n",
        "    return sha1.hexdigest()\n",
        "\n",
        "bucket_name = \"your-s3-bucket\"\n",
        "file_key = \"path/to/your/file.mp3\"\n",
        "\n",
        "# Get file content from S3 and calculate the SHA-1 checksum\n",
        "file_content, metadata = fetch_s3_metadata(bucket_name, file_key)\n",
        "checksum = generate_sha1_checksum(file_content)\n",
        "\n",
        "print(f\"SHA-1 Checksum for {file_key}: {checksum}\")\n"
      ],
      "metadata": {
        "id": "uXBaXpud84OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Box SDK for Metadata Fetching\n",
        "\n",
        "The Box SDK allows you to interact with Box to fetch metadata for files, including the SHA-1 checksum Box provides.\n",
        "Use Case:\n",
        "\n",
        "    Fetch metadata (including the SHA-1 checksum) from Box for comparison with files in S3."
      ],
      "metadata": {
        "id": "3xAXNuMtUB-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from boxsdk import Client, OAuth2\n",
        "\n",
        "# Authenticate with Box\n",
        "oauth2 = OAuth2(\n",
        "    client_id='your-client-id',\n",
        "    client_secret='your-client-secret',\n",
        "    access_token='your-access-token'\n",
        ")\n",
        "client = Client(oauth2)\n",
        "\n",
        "def get_box_file_metadata(file_id):\n",
        "    \"\"\"\n",
        "    Fetch metadata (including SHA-1) from Box for the given file.\n",
        "    \"\"\"\n",
        "    box_file = client.file(file_id).get()\n",
        "    sha1_checksum = box_file.sha1\n",
        "    file_name = box_file.name\n",
        "    file_size = box_file.size\n",
        "    return {\"name\": file_name, \"size\": file_size, \"sha1\": sha1_checksum}\n",
        "\n",
        "box_file_id = \"1234567890\"\n",
        "box_metadata = get_box_file_metadata(box_file_id)\n",
        "print(f\"Box File Metadata: {box_metadata}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "1IzMcHjL84LH",
        "outputId": "99a945a8-2b0e-4b3c-a09f-412803482ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'boxsdk'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bfed2aafc764>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mboxsdk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOAuth2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Authenticate with Box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m oauth2 = OAuth2(\n\u001b[1;32m      5\u001b[0m     \u001b[0mclient_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'your-client-id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boxsdk'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hashlib for SHA-1 Checksum Generation\n",
        "\n",
        "Hashlib is used to compute the SHA-1 checksum for the files fetched from S3.\n",
        "Use Case:\n",
        "\n",
        "    After fetching the file from S3, compute the SHA-1 checksum to compare with Box files."
      ],
      "metadata": {
        "id": "oHPhYwbLUMKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def generate_sha1_checksum(file_content):\n",
        "    sha1 = hashlib.sha1()\n",
        "    sha1.update(file_content)\n",
        "    return sha1.hexdigest()\n",
        "\n",
        "file_content = b\"Sample file content\"\n",
        "checksum = generate_sha1_checksum(file_content)\n",
        "print(f\"SHA-1 checksum: {checksum}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQfTr29d84IZ",
        "outputId": "49d88e44-ab72-4bdc-f262-1570a1cf0149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHA-1 checksum: 9fbd36649a852af83044e783b51245c31028aa31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splink for Probabilistic Matching and Deduplication\n",
        "\n",
        "Splink is a library for probabilistic record linkage and deduplication. It helps you match files between S3 and Box, even when exact matches are not possible (e.g., mismatched file names).\n",
        "Use Case:\n",
        "\n",
        "    Use Splink to compare metadata from S3 and Box and identify duplicates or similar files based on checksum, file name, or size.\n",
        "\n",
        "Steps:\n",
        "\n",
        "    Prepare data: Extract metadata from both S3 and Box and convert it to a DataFrame.\n",
        "    Define matching settings: Use Splink to compare file name, SHA-1 checksum, and size for deduplication.\n",
        "    Run Matching: Execute Splink to find exact or probabilistic matches."
      ],
      "metadata": {
        "id": "9J1Xs43yUW7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from splink import Splink\n",
        "\n",
        "# Example data from S3 and Box\n",
        "s3_data = pd.DataFrame([\n",
        "    {'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'file_name': 'audio2.mp3', 'sha1': 'XYZ456', 'size': 67890}\n",
        "])\n",
        "\n",
        "box_data = pd.DataFrame([\n",
        "    {'file_name': 'audio_duplicate.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'file_name': 'audio2.mp3', 'sha1': 'XYZ456', 'size': 67890}\n",
        "])\n",
        "\n",
        "# Linkage settings\n",
        "settings = {\n",
        "    \"link_type\": \"dedupe_only\",\n",
        "    \"comparison_columns\": [\n",
        "        {\"col_name\": \"file_name\", \"num_levels\": 2},\n",
        "        {\"col_name\": \"sha1\", \"num_levels\": 2},\n",
        "        {\"col_name\": \"size\", \"num_levels\": 2}\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Link S3 and Box data\n",
        "linker = Splink(settings, [s3_data, box_data])\n",
        "results = linker.get_match_candidates()\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "X5TUByxn84FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MongoDB for Metadata Storage\n",
        "\n",
        "Store all metadata from S3 and Box in MongoDB for querying and deduplication analysis.\n",
        "Use Case:\n",
        "\n",
        "    Use MongoDB to store the file metadata from S3 and Box, along with computed SHA-1 checksums, for easy querying and analysis.\n",
        "\n",
        "Steps:\n",
        "\n",
        "    Insert Metadata: After calculating SHA-1 checksums, store file metadata from S3 and Box into MongoDB.\n",
        "    Query Data: Use MongoDB queries to search for duplicates or near-duplicates based on SHA-1 or other criteria."
      ],
      "metadata": {
        "id": "tTT7SntMUepR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "# Connect to MongoDB\n",
        "client = MongoClient('mongodb://localhost:27017/')\n",
        "db = client['file_metadata_db']\n",
        "collection = db['file_metadata']\n",
        "\n",
        "# Insert metadata from S3\n",
        "s3_metadata = {\n",
        "    \"file_name\": \"audio1.mp3\",\n",
        "    \"sha1\": \"ABC123\",\n",
        "    \"size\": 12345,\n",
        "    \"source\": \"S3\"\n",
        "}\n",
        "collection.insert_one(s3_metadata)\n",
        "\n",
        "# Insert metadata from Box\n",
        "box_metadata = {\n",
        "    \"file_name\": \"audio_duplicate.mp3\",\n",
        "    \"sha1\": \"ABC123\",\n",
        "    \"size\": 12345,\n",
        "    \"source\": \"Box\"\n",
        "}\n",
        "collection.insert_one(box_metadata)\n",
        "\n",
        "# Query for duplicates based on SHA-1 checksum\n",
        "duplicates = collection.find({\"sha1\": \"ABC123\"})\n",
        "for doc in duplicates:\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "id": "KyUhhrwWUbJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python (Pandas) for Processing and Analysis\n",
        "\n",
        "Pandas will help you manipulate and analyze metadata from both S3 and Box. After identifying duplicates with Splink or SHA-1 checksum comparison, you can use Pandas for further processing.\n",
        "Use Case:\n",
        "\n",
        "    Use Pandas to analyze the file metadata, detect duplicates, and generate reports.\n",
        "\n",
        "Steps:\n",
        "\n",
        "    Load Metadata: Use Pandas to read and manipulate metadata from S3 and Box.\n",
        "    Analyze Duplicates: Use Pandas to detect duplicates based on SHA-1 or other attributes.\n",
        "    Generate Reports: Summarize the results (e.g., how many duplicates were found) for reporting purposes."
      ],
      "metadata": {
        "id": "YVWlqo-9Ux-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data from S3 and Box\n",
        "data = [\n",
        "    {'source': 'S3', 'file_name': 'audio1.mp3', 'sha1': 'ABC123', 'size': 12345},\n",
        "    {'source': 'Box', 'file_name': 'audio_duplicate.mp3', 'sha1': 'ABC123', 'size': 12345}\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Find duplicate files based on SHA-1\n",
        "duplicates = df[df.duplicated(subset='sha1', keep=False)]\n",
        "print(duplicates)"
      ],
      "metadata": {
        "id": "9GAZrEnzUbEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Workflow\n",
        "\n",
        "    S3 File Handling: Use AWS SDK (Boto3) to fetch file metadata and generate SHA-1 checksums.\n",
        "    Box Metadata: Use Box SDK to fetch file metadata from Box.\n",
        "    Checksum Comparison: Compare the SHA-1 checksums between S3 and Box files."
      ],
      "metadata": {
        "id": "490ysRRXU-_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When explaining your project and code to your professor, it's important to present it in a clear, structured, and logical manner. Here's how you can explain the steps, methodology, and choices you made for this deduplication and record linkage project using **Splink** in **Google Colab**.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Introduction to the Problem**\n",
        "Start by explaining the context and what the assignment is about.\n",
        "\n",
        "**Example:**\n",
        "\"Professor, the goal of this project is to identify duplicate media assets between two different systems: **OneDAM** and **Box**. Since these systems may have different ways of calculating checksums (e.g., MD5 and SHA-1), and even different file formats for the same content, we are using probabilistic record linkage to match records. The records are provided as metadata in JSON format, and I'm using a Python library called **Splink** for probabilistic linkage.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Why Use Probabilistic Linkage?**\n",
        "Highlight why deterministic methods (like direct comparison of checksums) may not work and why a probabilistic approach is more appropriate.\n",
        "\n",
        "**Example:**\n",
        "\"Checksum values like MD5 and SHA-1 are not entirely reliable due to collision attacks, meaning different files can sometimes produce the same checksum. To avoid false matches, I opted for **probabilistic linkage**, which compares multiple attributes (like filenames, checksums, creation dates, etc.) and assigns a probability to how likely two records are duplicates. This gives us a more flexible and accurate way of identifying duplicates.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Tools and Libraries Used**\n",
        "Briefly explain the tools you chose and why.\n",
        "\n",
        "**Example:**\n",
        "\"I used **Google Colab** for coding because it provides a convenient, cloud-based environment that can handle Python code. For the linkage itself, I used the **Splink** library. This library is designed for probabilistic record linkage and is highly configurable. It allowed me to compare metadata fields like filenames, checksums, and modification dates to detect duplicates.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Steps Followed in the Project**\n",
        "Now walk through the workflow, explaining the key steps.\n",
        "\n",
        "#### Step 1: **Loading the Metadata**\n",
        "\"I started by loading the metadata from **OneDAM** and **Box** into Python. The metadata was in JSON format, so I used Python’s `json` and `pandas` libraries to read and convert this data into a structured format (Pandas DataFrame), which is easier to work with.\"\n",
        "\n",
        "#### Step 2: **Combining the Data**\n",
        "\"Next, I combined the metadata from both systems into a single dataset. This allowed me to compare records across both systems.\"\n",
        "\n",
        "#### Step 3: **Configuring Splink for Record Linkage**\n",
        "\"To set up the probabilistic linkage, I defined which fields in the metadata should be compared. I chose to compare fields like `filename`, `checksum`, `creation_date`, and `modification_date`. In Splink, you can assign different weights to these fields based on how important they are for matching.\"\n",
        "\n",
        "#### Step 4: **Running the Linkage Process**\n",
        "\"Splink then runs the comparison, assigning a probability score to each potential match. This score helps us determine whether two records from OneDAM and Box are likely to be duplicates.\"\n",
        "\n",
        "#### Step 5: **Identifying and Saving the Duplicates**\n",
        "\"Once the linkage process was complete, I extracted the results into a Pandas DataFrame, where I could see which records were flagged as potential duplicates. I then saved the results to a CSV file for further analysis.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Results and What They Mean**\n",
        "Summarize what you achieved.\n",
        "\n",
        "**Example:**\n",
        "\"The result of this process is a set of potential duplicate records across OneDAM and Box, each with a probability score indicating how likely it is that they are duplicates. This approach ensures that we capture duplicates even when simple checksum validation wouldn't have worked.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Challenges and Solutions**\n",
        "Mention any challenges you faced and how you overcame them.\n",
        "\n",
        "**Example:**\n",
        "\"One challenge was that different hash algorithms, like MD5 and SHA-1, could return the same values for different files. To address this, I configured Splink to look at multiple metadata fields and used **probabilistic** rather than deterministic matching.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Why Google Colab?**\n",
        "Explain why you chose Google Colab as your development environment.\n",
        "\n",
        "**Example:**\n",
        "\"I used Google Colab because it's a free, cloud-based environment that provides access to all necessary Python libraries and tools. It’s also convenient for uploading files and sharing results.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Potential Improvements**\n",
        "You could also mention what could be improved in future work.\n",
        "\n",
        "**Example:**\n",
        "\"In the future, I could refine the linkage model by including additional metadata fields if available (e.g., file size, MIME type). Additionally, I could tune the thresholds for better precision in detecting duplicates.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Code Walkthrough**\n",
        "Be ready to walk your professor through the key parts of the code:\n",
        "\n",
        "- **Loading the Data**: \"Here, I use `pandas` and `json` to load the JSON files into DataFrames.\"\n",
        "- **Splink Configuration**: \"I defined which fields Splink should compare, such as `filename` and `checksum`.\"\n",
        "- **Running the Model**: \"Splink then calculates match probabilities for each record pair.\"\n",
        "- **Outputting Results**: \"Finally, I saved the results to a CSV file for analysis.\"\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **Conclusion**\n",
        "Wrap up by summarizing the main points and what you've learned.\n",
        "\n",
        "**Example:**\n",
        "\"In conclusion, this project demonstrates how probabilistic linkage can effectively identify duplicate files across systems even when simple checksum comparisons fail. Using **Splink** in **Google Colab** allowed me to build a robust process for deduplication that can be extended to larger datasets in the future.\"\n",
        "\n",
        "---\n",
        "\n",
        "### Bonus: **Potential Questions**\n",
        "Be prepared for questions like:\n",
        "- **Why did you choose these specific fields for linkage?**\n",
        "  - Explain why fields like `filename` and `checksum` were used—because they are crucial for file identity.\n",
        "  \n",
        "- **How accurate is this process?**\n",
        "  - You could explain that Splink provides probability scores, and the accuracy depends on the quality of the metadata and how the model is configured.\n",
        "\n",
        "- **How could this be scaled to larger datasets?**\n",
        "  - Mention that Splink can handle large datasets, and with cloud-based environments like Colab, you can scale up by integrating with more powerful services if needed.\n",
        "\n",
        "---\n",
        "\n",
        "By following this structure, you can clearly explain your project, the reasoning behind your choices, and how you implemented the solution using Splink in Google Colab. Let me know if you'd like more detail on any of the sections!"
      ],
      "metadata": {
        "id": "t0b5B1e-I4YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install splink pandas pyarrow"
      ],
      "metadata": {
        "id": "iFko_fhAJO8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install splink"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF8MDV8Tmvww",
        "outputId": "7fa7de4b-d083-4317-f451-83873b87a2fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: splink in /usr/local/lib/python3.10/dist-packages (4.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from splink) (3.1.4)\n",
            "Requirement already satisfied: altair<6.0.0,>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from splink) (5.4.1)\n",
            "Requirement already satisfied: duckdb>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from splink) (1.1.2)\n",
            "Requirement already satisfied: igraph>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from splink) (0.11.6)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from splink) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from splink) (1.26.4)\n",
            "Requirement already satisfied: pandas>1.3.5 in /usr/local/lib/python3.10/dist-packages (from splink) (2.2.2)\n",
            "Requirement already satisfied: sqlglot>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from splink) (25.1.0)\n",
            "Requirement already satisfied: narwhals>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (1.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (4.12.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph>=0.11.2->splink) (1.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->splink) (3.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>1.3.5->splink) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the two JSON metadata files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "5jDDr531Ua9k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "55e7d502-ab7d-4baa-f3a2-5bd436ade727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ae7c897-1c74-4792-8da4-1bea62a2d3f8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ae7c897-1c74-4792-8da4-1bea62a2d3f8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BoxData.ipynb to BoxData (1).ipynb\n",
            "Saving onddam_examole_data.json to onddam_examole_data (1).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from splink import Splink\n",
        "from splink.settings import complete_settings_link_only\n",
        "\n",
        "# Load JSON data from OneDAM and Box\n",
        "with open('oneDAM_metadata.json') as f1, open('box_metadata.json') as f2:\n",
        "    oneDAM_data = json.load(f1)\n",
        "    box_data = json.load(f2)\n",
        "\n",
        "# Convert JSON data into Pandas DataFrames\n",
        "df_oneDAM = pd.DataFrame(oneDAM_data)\n",
        "df_box = pd.DataFrame(box_data)\n",
        "\n",
        "# Combine datasets (for linkage)\n",
        "df_combined = pd.concat([df_oneDAM, df_box], keys=[\"OneDAM\", \"Box\"])\n",
        "\n",
        "# Splink linkage configuration\n",
        "linking_fields = [\n",
        "    {\"col_name\": \"filename\", \"term_frequency_adjustments\": True},\n",
        "    {\"col_name\": \"checksum\"},\n",
        "    {\"col_name\": \"creation_date\"},\n",
        "    {\"col_name\": \"modification_date\"}\n",
        "]\n",
        "\n",
        "# Configure Splink settings for probabilistic record linkage\n",
        "settings = complete_settings_link_only(linking_fields)\n",
        "\n",
        "# Initialize Splink model with combined dataset\n",
        "model = Splink(settings, df_combined)\n",
        "\n",
        "# Run the linkage process to find duplicates\n",
        "results = model.run_linking()\n",
        "\n",
        "# Convert the results to a Pandas DataFrame\n",
        "duplicates = results.as_pandas_dataframe()\n",
        "\n",
        "# Show the first few records of possible duplicates\n",
        "print(duplicates.head())"
      ],
      "metadata": {
        "id": "-FdkfqYdUa6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "47af4827-337a-4a95-f8c8-8df219cf42b8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Splink' from 'splink' (/usr/local/lib/python3.10/dist-packages/splink/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e136207baab3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomplete_settings_link_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Splink' from 'splink' (/usr/local/lib/python3.10/dist-packages/splink/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "# Import Splink from splink.linker\n",
        "from splink.linker import Linker as Splink  # Splink class is now called Linker\n",
        "from splink.settings import complete_settings_link_only\n",
        "# Load JSON data from OneDAM and Box\n",
        "with open('oneDAM_metadata.json') as f1, open('box_metadata.json') as f2:\n",
        "    oneDAM_data = json.load(f1)\n",
        "    box_data = json.load(f2)\n",
        "\n",
        "# Convert JSON data into Pandas DataFrames\n",
        "df_oneDAM = pd.DataFrame(oneDAM_data)\n",
        "df_box = pd.DataFrame(box_data)\n",
        "\n",
        "# Combine datasets (for linkage)\n",
        "df_combined = pd.concat([df_oneDAM, df_box], keys=[\"OneDAM\", \"Box\"])\n",
        "\n",
        "# Splink linkage configuration\n",
        "linking_fields = [\n",
        "    {\"col_name\": \"filename\", \"term_frequency_adjustments\": True},\n",
        "    {\"col_name\": \"checksum\"},\n",
        "    {\"col_name\": \"creation_date\"},\n",
        "    {\"col_name\": \"modification_date\"}\n",
        "]\n",
        "\n",
        "# Configure Splink settings for probabilistic record linkage\n",
        "settings = complete_settings_link_only(linking_fields)\n",
        "\n",
        "# Initialize Splink model with combined dataset\n",
        "model = Splink(settings, df_combined)\n",
        "\n",
        "# Run the linkage process to find duplicates\n",
        "results = model.run_linking()\n",
        "\n",
        "# Convert the results to a Pandas DataFrame\n",
        "duplicates = results.as_pandas_dataframe()\n",
        "\n",
        "# Show the first few records of possible duplicates\n",
        "print(duplicates.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "WdD_VCqPmRdj",
        "outputId": "eb0ce6ba-cf0a-44ce-ea3c-a6946e46e493",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'splink.linker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-35ae54084309>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import Splink from splink.linker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinker\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSplink\u001b[0m  \u001b[0;31m# Splink class is now called Linker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomplete_settings_link_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load JSON data from OneDAM and Box\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'splink.linker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "duplicates.to_csv('duplicates_results.csv', index=False)\n",
        "\n",
        "# Download the file\n",
        "from google.colab import files\n",
        "files.download('duplicates_results.csv')"
      ],
      "metadata": {
        "id": "sdTC016sUa3F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "e7435997-82a9-47c2-adca-b145698f978a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'duplicates' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fc680f071bfd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save the results to a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mduplicates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'duplicates_results.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Download the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'duplicates' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install splink pandas pyarrow"
      ],
      "metadata": {
        "id": "Lko00Di0Uaz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8048bb2-5dea-4789-b915-5d6b37f84141",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting splink\n",
            "  Downloading splink-4.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (16.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from splink) (3.1.4)\n",
            "Collecting altair<6.0.0,>=5.0.1 (from splink)\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: duckdb>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from splink) (1.1.2)\n",
            "Collecting igraph>=0.11.2 (from splink)\n",
            "  Downloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from splink) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from splink) (1.26.4)\n",
            "Requirement already satisfied: sqlglot>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from splink) (25.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting narwhals>=1.5.2 (from altair<6.0.0,>=5.0.1->splink)\n",
            "  Downloading narwhals-1.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (4.12.2)\n",
            "Collecting texttable>=1.6.2 (from igraph>=0.11.2->splink)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->splink) (3.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (0.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading splink-4.0.4-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.9.4-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.9/188.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, narwhals, igraph, altair, splink\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed altair-5.4.1 igraph-0.11.6 narwhals-1.9.4 splink-4.0.4 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the two JSON metadata files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "sAI-p7zOoAo4",
        "outputId": "e65840e5-10a4-4ea1-f5c2-7b3bbc93d80d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e0d95b20-a1fb-4b78-af1d-522bd857b447\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e0d95b20-a1fb-4b78-af1d-522bd857b447\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BoxData.ipynb to BoxData (2).ipynb\n",
            "Saving onddam_examole_data.json to onddam_examole_data (2).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from splink import Linker\n",
        "from splink.settings import complete_settings_link_only\n",
        "\n",
        "# Load JSON data from the uploaded files\n",
        "oneDAM_data = json.loads(uploaded['oneDAM_metadata.json'])\n",
        "box_data = json.loads(uploaded['box_metadata.json'])\n",
        "\n",
        "# Convert JSON data into Pandas DataFrames\n",
        "df_oneDAM = pd.DataFrame(oneDAM_data)\n",
        "df_box = pd.DataFrame(box_data)\n",
        "\n",
        "# Combine datasets (for linkage)\n",
        "df_combined = pd.concat([df_oneDAM, df_box], keys=[\"OneDAM\", \"Box\"])\n",
        "\n",
        "# Splink linkage configuration\n",
        "linking_fields = [\n",
        "    {\"col_name\": \"filename\", \"term_frequency_adjustments\": True},\n",
        "    {\"col_name\": \"checksum\"},\n",
        "    {\"col_name\": \"creation_date\"},\n",
        "    {\"col_name\": \"modification_date\"}\n",
        "]\n",
        "\n",
        "# Configure Splink settings for probabilistic record linkage\n",
        "settings = complete_settings_link_only(linking_fields)\n",
        "\n",
        "# Initialize Splink model with combined dataset\n",
        "linker = Linker(settings, df_combined)\n",
        "\n",
        "# Run the linkage process to find duplicates\n",
        "results = linker.get_duplicates()\n",
        "\n",
        "# Convert the results to a Pandas DataFrame\n",
        "duplicates = results.as_pandas_dataframe()\n",
        "\n",
        "# Show the first few records of possible duplicates\n",
        "print(duplicates.head())\n",
        "\n",
        "# Save the results to a CSV file\n",
        "duplicates.to_csv('duplicates_results.csv', index=False)\n",
        "\n",
        "# Download the file\n",
        "files.download('duplicates_results.csv')\n"
      ],
      "metadata": {
        "id": "Oo3Fg4n8oDJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install splink[spark] pandas duckdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWYiMArQtXPZ",
        "outputId": "6705a755-4f0e-4a86-a878-87065fefa7ca",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: splink[spark] in /usr/local/lib/python3.10/dist-packages (4.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (3.1.4)\n",
            "Requirement already satisfied: altair<6.0.0,>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (5.4.1)\n",
            "Requirement already satisfied: igraph>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (0.11.6)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (1.26.4)\n",
            "Requirement already satisfied: sqlglot>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (25.1.0)\n",
            "Requirement already satisfied: pyspark>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (3.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: narwhals>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[spark]) (1.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[spark]) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[spark]) (4.12.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph>=0.11.2->splink[spark]) (1.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->splink[spark]) (3.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (0.20.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark>=3.2.1->splink[spark]) (0.10.9.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "CjDMaH-VtXMZ",
        "outputId": "3243b250-e698-43a3-d903-aba5d03f4bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5faed201-26f2-4521-b114-7b7e10e5cd62\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5faed201-26f2-4521-b114-7b7e10e5cd62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BoxData.ipynb to BoxData.ipynb\n",
            "Saving onddam_examole_data.json to onddam_examole_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the uploaded files into Pandas DataFrames (assuming they are JSON files)\n",
        "oneDAM_df = pd.read_json(list(BoxData.ipynb.keys())[0])  # Replace with actual file name\n",
        "box_df = pd.read_json(list(onddam_examole_data.json.keys())[1])  # Replace with actual file name\n",
        "\n",
        "# Add a column to indicate the source system\n",
        "oneDAM_df['system'] = 'OneDAM'\n",
        "box_df['system'] = 'Box'\n",
        "\n",
        "# Combine both datasets into a single DataFrame\n",
        "df_combined = pd.concat([oneDAM_df, box_df], ignore_index=True)\n",
        "\n",
        "# Preview the data\n",
        "df_combined.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "B3PR_AzGtXJ8",
        "outputId": "10b87e1a-d061-4fb9-f9df-f120c5abb99e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BoxData' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-345613eb1e41>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the uploaded files into Pandas DataFrames (assuming they are JSON files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moneDAM_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBoxData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with actual file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbox_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monddam_examole_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with actual file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Add a column to indicate the source system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BoxData' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now yoiu need to load the uploaded files into Pandas DataFrames (assuming they are JSON files)\n",
        "oneDAM_df = pd.read_json(list(uploaded.keys())[0])  # whatever the file you have in oneDAm just replace with actual file name\n",
        "box_df = pd.read_json(list(uploaded.keys())[1])  # whatever the file you have in box replace with actual file name\n",
        "\n",
        "# Now add a column to indicate the source system\n",
        "oneDAM_df['system'] = 'OneDAM'\n",
        "box_df['system'] = 'Box'"
      ],
      "metadata": {
        "id": "7M03qKEeN9KA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "03e2e9ce-6bc9-4db7-f290-f60313937a1c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-09ec02383a5f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now yoiu need to load the uploaded files into Pandas DataFrames (assuming they are JSON files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moneDAM_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# whatever the file you have in oneDAm just replace with actual file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbox_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# whatever the file you have in box replace with actual file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Now add a column to indicate the source system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after adding column we need to combine both datasets into a single DataFrame for further works\n",
        "df_combined = pd.concat([oneDAM_df, box_df], ignore_index=True)\n",
        "\n",
        "# Preview the data\n",
        "df_combined.head()"
      ],
      "metadata": {
        "id": "gpTH4-QUN9Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**# If above given code is not working then you can try this one:**\n"
      ],
      "metadata": {
        "id": "ZX5S6j8bOlFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example metadata for OneDAM\n",
        "oneDAM_data = [\n",
        "    {\"file_name\": \"fileA.mp4\", \"file_size\": 1000, \"file_type\": \"video\", \"checksum\": \"md5abc\", \"created_date\": \"2023-01-01\"},\n",
        "    {\"file_name\": \"fileB.mp3\", \"file_size\": 500, \"file_type\": \"audio\", \"checksum\": \"md5xyz\", \"created_date\": \"2022-12-15\"}\n",
        "]\n",
        "\n",
        "# Example metadata for Box\n",
        "box_data = [\n",
        "    {\"file_name\": \"fileA_renamed.mp4\", \"file_size\": 1000, \"file_type\": \"video\", \"checksum\": \"sha1abc\", \"created_date\": \"2023-01-02\"},\n",
        "    {\"file_name\": \"fileC.mp3\", \"file_size\": 600, \"file_type\": \"audio\", \"checksum\": \"sha1xyz\", \"created_date\": \"2022-11-20\"}\n",
        "]\n",
        "\n",
        "# Convert the data to DataFrames\n",
        "oneDAM_df = pd.DataFrame(oneDAM_data)\n",
        "box_df = pd.DataFrame(box_data)\n",
        "\n",
        "# Add a system column\n",
        "oneDAM_df['system'] = 'OneDAM'\n",
        "box_df['system'] = 'Box'\n",
        "\n",
        "# Combine both datasets into a single DataFrame\n",
        "df_combined = pd.concat([oneDAM_df, box_df], ignore_index=True)\n",
        "\n",
        "# Preview the combined data\n",
        "df_combined.head()\n"
      ],
      "metadata": {
        "id": "Xkac5wmrOkuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now in next step we need to configure splink linkage configure and run with prepared data.\n"
      ],
      "metadata": {
        "id": "qPt4SkLEOwlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from splink.duckdb.duckdb_linker import DuckDBLinker\n",
        "from splink import Splink\n",
        "\n",
        "# Define Splink settings for probabilistic linkage\n",
        "settings = {\n",
        "    \"link_type\": \"link_and_dedupe\",  # We want to dedupe and link across the two systems\n",
        "    \"blocking_rules_to_generate_predictions\": [\n",
        "        \"l.file_size = r.file_size\"  # Block by file size to limit comparisons\n",
        "    ],\n",
        "    \"comparison_columns\": [\n",
        "        {\n",
        "            \"col_name\": \"file_name\",\n",
        "            \"num_levels\": 3,  # Fuzzy matching with multiple levels\n",
        "            \"term_frequency_adjustments\": True\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"file_size\",\n",
        "            \"num_levels\": 1,  # Exact match for file size\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"file_type\",\n",
        "            \"num_levels\": 1,  # Exact match for file type\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"checksum\",\n",
        "            \"num_levels\": 1,  # Exact match for checksum (even though algorithms differ, it's worth trying)\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"system\",  # Ensure we only compare files across systems (OneDAM vs Box)\n",
        "            \"num_levels\": 1,\n",
        "            \"case_expression\": \"l.system != r.system\"\n",
        "        }\n",
        "    ],\n",
        "    \"probability_two_random_records_match\": 0.01,  # Small likelihood of a random match\n",
        "}\n"
      ],
      "metadata": {
        "id": "9HJk1CX-N9Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install splink[duckdb]\n",
        "# Installs splink with DuckDB support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PErQRL94RMuZ",
        "outputId": "33bb574b-be10-4d55-fd50-ad190cbc79d7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: splink[duckdb] in /usr/local/lib/python3.10/dist-packages (4.0.4)\n",
            "\u001b[33mWARNING: splink 4.0.4 does not provide the extra 'duckdb'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (3.1.4)\n",
            "Requirement already satisfied: altair<6.0.0,>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (5.4.1)\n",
            "Requirement already satisfied: duckdb>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (1.1.2)\n",
            "Requirement already satisfied: igraph>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (0.11.6)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (1.26.4)\n",
            "Requirement already satisfied: pandas>1.3.5 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (2.2.2)\n",
            "Requirement already satisfied: sqlglot>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from splink[duckdb]) (25.1.0)\n",
            "Requirement already satisfied: narwhals>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[duckdb]) (1.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[duckdb]) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[duckdb]) (4.12.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph>=0.11.2->splink[duckdb]) (1.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->splink[duckdb]) (3.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[duckdb]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[duckdb]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[duckdb]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[duckdb]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink[duckdb]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink[duckdb]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink[duckdb]) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>1.3.5->splink[duckdb]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from splink.duckdb.duckdb_linker import DuckDBLinker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "zvXZvZw3RMrh",
        "outputId": "16a33d89-06d8-47cd-9b7a-5975d999d84d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'splink.duckdb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-74faf6a5a24c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduckdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduckdb_linker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDuckDBLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'splink.duckdb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now next we need to initialize the Splink DuckDB linker\n",
        "linker = DuckDBLinker(df_combined, settings)\n",
        "\n",
        "# Run the linkage process"
      ],
      "metadata": {
        "id": "oouSS3oYtXDk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "6afd7baf-c3e7-4690-8813-2168c31e24ca",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DuckDBLinker' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-54ca22463e0c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now next we need to initialize the Splink DuckDB linker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDuckDBLinker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run the linkage process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DuckDBLinker' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can try to predict matches\n",
        "df_predictions = linker.predict()\n",
        "\n",
        "# Display the matching results\n",
        "df_predictions[['l.file_name', 'r.file_name', 'match_probability']].head()"
      ],
      "metadata": {
        "id": "oledynvFPKUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can put all together in app.py if you arebtrying vs code\n",
        "\n"
      ],
      "metadata": {
        "id": "-qrLVCMqPREh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install splink[spark] pandas duckdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_ITpdPLQn4R",
        "outputId": "cd13d078-0256-4f5d-8aef-c6baf3595752",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Collecting splink[spark]\n",
            "  Downloading splink-4.0.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (3.1.4)\n",
            "Collecting altair<6.0.0,>=5.0.1 (from splink[spark])\n",
            "  Downloading altair-5.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting igraph>=0.11.2 (from splink[spark])\n",
            "  Downloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (1.26.4)\n",
            "Requirement already satisfied: sqlglot>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (25.1.0)\n",
            "Requirement already satisfied: pyspark>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from splink[spark]) (3.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Collecting narwhals>=1.5.2 (from altair<6.0.0,>=5.0.1->splink[spark])\n",
            "  Downloading narwhals-1.9.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[spark]) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink[spark]) (4.12.2)\n",
            "Collecting texttable>=1.6.2 (from igraph>=0.11.2->splink[spark])\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->splink[spark]) (3.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink[spark]) (0.20.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark>=3.2.1->splink[spark]) (0.10.9.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading altair-5.4.1-py3-none-any.whl (658 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m658.1/658.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading splink-4.0.4-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.9.4-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.9/188.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, narwhals, igraph, altair, splink\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.2\n",
            "    Uninstalling altair-4.2.2:\n",
            "      Successfully uninstalled altair-4.2.2\n",
            "Successfully installed altair-5.4.1 igraph-0.11.6 narwhals-1.9.4 splink-4.0.4 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install splink duckdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nBDmR8IRCtH",
        "outputId": "628d0057-26cf-4b17-81ba-72885740bd9a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: splink in /usr/local/lib/python3.10/dist-packages (4.0.4)\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from splink) (3.1.4)\n",
            "Requirement already satisfied: altair<6.0.0,>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from splink) (5.4.1)\n",
            "Requirement already satisfied: igraph>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from splink) (0.11.6)\n",
            "Requirement already satisfied: jsonschema>=3.2 in /usr/local/lib/python3.10/dist-packages (from splink) (4.23.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from splink) (1.26.4)\n",
            "Requirement already satisfied: pandas>1.3.5 in /usr/local/lib/python3.10/dist-packages (from splink) (2.2.2)\n",
            "Requirement already satisfied: sqlglot>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from splink) (25.1.0)\n",
            "Requirement already satisfied: narwhals>=1.5.2 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (1.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0.0,>=5.0.1->splink) (4.12.2)\n",
            "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph>=0.11.2->splink) (1.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0.3->splink) (3.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.2->splink) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>1.3.5->splink) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>1.3.5->splink) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from splink.duckdb.duckdb_linker import DuckDBLinker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "IdyhbH73RCp6",
        "outputId": "22de1311-70ad-4af8-ce95-f228416c3ca8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'splink.duckdb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-74faf6a5a24c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduckdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduckdb_linker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDuckDBLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'splink.duckdb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from splink.duckdb.duckdb_linker import DuckDBLinker\n",
        "from splink import Splink\n",
        "\n",
        "# Define Splink settings for probabilistic linkage\n",
        "settings = {\n",
        "    \"link_type\": \"link_and_dedupe\",  # We want to dedupe and link across the two systems\n",
        "    \"blocking_rules_to_generate_predictions\": [\n",
        "        \"l.file_size = r.file_size\"  # Block by file size to limit comparisons\n",
        "    ],\n",
        "    \"comparison_columns\": [\n",
        "        {\n",
        "            \"col_name\": \"file_name\",\n",
        "            \"num_levels\": 3,  # Fuzzy matching with multiple levels\n",
        "            \"term_frequency_adjustments\": True\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"file_size\",\n",
        "            \"num_levels\": 1,  # Exact match for file size\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"file_type\",\n",
        "            \"num_levels\": 1,  # Exact match for file type\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"checksum\",\n",
        "            \"num_levels\": 1,  # Exact match for checksum (even though algorithms differ, it's worth trying)\n",
        "        },\n",
        "        {\n",
        "            \"col_name\": \"system\",  # Ensure we only compare files across systems (OneDAM vs Box)\n",
        "            \"num_levels\": 1,\n",
        "            \"case_expression\": \"l.system != r.system\"\n",
        "        }\n",
        "    ],\n",
        "    \"probability_two_random_records_match\": 0.01,  # Small likelihood of a random match\n",
        "}\n",
        "\n",
        "# Initialize the Splink DuckDB linker\n",
        "linker = DuckDBLinker(df_combined, settings)\n",
        "\n",
        "# Predict matches\n",
        "df_predictions = linker.predict()\n",
        "\n",
        "# Display the matching results\n",
        "df_predictions[['l.file_name', 'r.file_name', 'match_probability']].head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "z2-lyAtPPQwz",
        "outputId": "6952c74f-899f-4a9a-de05-b6a14f577252",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'splink.duckdb'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b432d5e67755>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduckdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduckdb_linker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDuckDBLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define Splink settings for probabilistic linkage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m settings = {\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'splink.duckdb'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# in next step we need to filter and analyze the matching results\n",
        "for this you can try filtering the results based on the match probability score to get list of valid/potential duplicate files across both box and od."
      ],
      "metadata": {
        "id": "WdR06blzPjsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter results based on match probability (set a threshold, e.g., 0.95)\n",
        "df_matches = df_predictions[df_predictions['match_probability'] > 0.95]\n",
        "\n",
        "# Show matched pairs of files across OneDAM and Box\n",
        "df_matches[['l.file_name', 'r.file_name', 'l.system', 'r.system', 'match_probability']]"
      ],
      "metadata": {
        "id": "i_90VlEQPjhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can now try with analyzing duplicate files on fields based on given schema such as created date, modified date, other metadta.\n",
        "\n",
        "# make sure that your data is clean, no missing value and normalized."
      ],
      "metadata": {
        "id": "s8Li54B0QGFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis for duplicates\n",
        "duplicates = df_matches[df_matches['match_probability'] > 0.95]\n",
        "\n",
        "# Print details of duplicates\n",
        "print(duplicates[['l.file_name', 'r.file_name', 'l.created_date', 'r.created_date']])"
      ],
      "metadata": {
        "id": "IEfaQVlGPizJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}